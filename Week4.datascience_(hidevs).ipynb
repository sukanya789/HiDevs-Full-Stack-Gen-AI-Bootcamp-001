{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a33e85fd",
   "metadata": {},
   "source": [
    "# Unlocking Insights of Data Science using Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a22bd09",
   "metadata": {},
   "source": [
    "# 1. Wine Review Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bc95c5",
   "metadata": {},
   "source": [
    "This project involves the analysis of wine and to achieve this goal, the project involves several steps here I will mention few important steps to achive(for complete modelling project click to the path- https://github.com/sukanya789/ML_practice_project/blob/main/redwine_quality_analysis.ipynb). Initially starting with setting up data science environment, proceeds with data manipulation or preprocessing with pandas, Descriptive analysis and matrix calculation with NumPy, and Data Visualization with Matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21503de7",
   "metadata": {},
   "source": [
    "a. SetUp Data Science Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960f21fb",
   "metadata": {},
   "source": [
    "* Using Terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fe7283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigating to the directory where we want to store notebook files\n",
    "cd /path/to/your/project/directory\n",
    "\n",
    "# Create a virtual environment\n",
    "python -m venv dsenv\n",
    "\n",
    "# Activate VE\n",
    "source dsenv/bin/activate\n",
    "\n",
    "# Install necessary libraries\n",
    "pip install pandas numpy matplotlib scikit-learn jupyterlab\n",
    "\n",
    "# Start Jupyter notebook\n",
    "jupyter notebbook\n",
    "\n",
    "# Deactivate(at the completion of task)\n",
    "deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e48130",
   "metadata": {},
   "source": [
    "* Using local environment like Jupyter Notebook & Google Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ea8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "    \n",
    "# Install VirEnv if not already installed and Set up VirEnv if necessary \n",
    "\n",
    "try:\n",
    "    import virtualenv\n",
    "except ImportError:\n",
    "    install('virtualenv')\n",
    "    \n",
    "# Install Necessary Libraries\n",
    "libraries = ['pandas', 'numpy', 'matplotlib']\n",
    "for lib in libraries:\n",
    "    install(lib)\n",
    "    \n",
    "# Create a VirEnv\n",
    "subprocess.run(['virtualenv', 'dsenv'])\n",
    "\n",
    "# Activate\n",
    "if sys.platform.startwith('win'):\n",
    "    activate_script = 'dsenv\\\\Scripts\\\\activate_this.py'\n",
    "else:\n",
    "    activate_script = 'dsenv/bin/activate_this.py'\n",
    "    \n",
    "with open(activate_script, \"r\") as file:\n",
    "    exec(file.read(), dict(__file__ = activate_script))\n",
    "        \n",
    "print(\"Data Science Environment Setup Complete.\")       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a97729",
   "metadata": {},
   "source": [
    "b. Data Manipulation with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad498e45",
   "metadata": {},
   "source": [
    "Here I am presenting some basic steps of this step of modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a3cd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load dataset\n",
    "df = pd.read_csv('') # saved dataset name\n",
    "\n",
    "# Perform basic data exploration and preprocessing steps\n",
    "print(df.head())  # Print the first few rows of the DataFrame\n",
    "print(df.dtypes)  # Print the data types of each column in the DataFrame\n",
    "print(df.describe()) # Generate summary statistics of the DataFrame\n",
    "print(df.isnull().sum()) # Count missing values in each column of the DataFrame\n",
    "print(df.loc[df['column_name'] > value])  # Filter rows based on a condition\n",
    "print(df.sort_values(by='column_name', ascending=False))  # Sort DataFrame by a column\n",
    "print(df.groupby('grouping_column').mean())  # Group data and calculate the mean\n",
    "\n",
    "# data preprocessing\n",
    "df = df.dropna() # drop columns/rows with missing values\n",
    "df = df.drop_duplicates() #remove duplicates rows\n",
    "merged_df = pd.merge(df1, df2, on='key_column')  # Merge two DataFrames\n",
    "df.fillna(value, inplace=True)  # Fill missing values with specified value\n",
    "df['new_column'] = df['column_one'] * df['column_two']  # Perform element-wise multiplication to create a new column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb16f2a",
   "metadata": {},
   "source": [
    "c. Descriptive Analysis and Matrix calculation with NumPy (As per requirement of dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ab5c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Descriptive Analysis\n",
    "print(\"Descriptive Analysis:\")\n",
    "print(\"Mean:\", np.mean(data, axis=0))\n",
    "print(\"Median:\", np.median(data, axis=0))\n",
    "print(\"Standard Deviation:\", np.std(data, axis=0))\n",
    "\n",
    "# For matrix calculations\n",
    "\n",
    "# Calculate the covariance matrix\n",
    "cov_matrix = np.cov(X.T)\n",
    "print(\"\\nCovariance Matrix:\")\n",
    "print(cov_matrix)\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = np.corrcoef(X.T)\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f4565c",
   "metadata": {},
   "source": [
    "d. Data visualtization with Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378db9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot Graphs and maps for visualization\n",
    "\n",
    "# Histogram of a feature (e.g., 'alcohol')\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(df['alcohol'], bins=30, color='purple', alpha=0.7)\n",
    "plt.title('Distribution of Alcohol in % Vol')\n",
    "plt.xlabel('Alcohol in % Vol')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of two features (e.g., 'alcohol' and 'quality')\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(df['alcohol'], df['quality'], color='blue', alpha=0.7)\n",
    "plt.title('Alcohol vs Quality')\n",
    "plt.xlabel('Alcohol in % Vol')\n",
    "plt.ylabel('Quality')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Box plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.boxplot(df['alcohol'], notch=True, vert=False)\n",
    "plt.title('Box Plot of Alcohol in % Vol')\n",
    "plt.xlabel('Alcohol in % Vol')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Bar plot, let's assume 'quality' is a categorical variable\n",
    "quality_counts = df['quality'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(quality_counts.index, quality_counts.values, color='green', alpha=0.7)\n",
    "plt.title('Bar Plot of Wine Quality')\n",
    "plt.xlabel('Quality')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Pie chart\n",
    "quality_counts = df['quality'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.pie(quality_counts.values, labels=quality_counts.index, autopct='%1.1f%%')\n",
    "plt.title('Pie Chart of Wine Quality')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7a94cb",
   "metadata": {},
   "source": [
    " # 2. Housing Price Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe2b384",
   "metadata": {},
   "source": [
    "The aim of this project is to predict housing prices using various features. This project will help us understand the application of data science in real estate markets and how different factors influence the price of a house.\n",
    "Here we will see the steps to perform data preprocessing, descriptive analysis, data visualization, feature engineering, how to train a model to predict the housing prices. We will also see how to evaluate the performance of  model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e597e6",
   "metadata": {},
   "source": [
    "a. Data Manipulation /Preprocessing with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7244dab8",
   "metadata": {},
   "source": [
    "This steps is almost same for every machine learning (data science) project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164a3cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load dataset\n",
    "df = pd.read_csv('') # saved dataset name\n",
    "\n",
    "# Perform basic data exploration and preprocessing steps\n",
    "print(df.head())  \n",
    "print(df.dtypes)  \n",
    "print(df.describe()) \n",
    "print(df.isnull().sum()) \n",
    "print(df.loc[df['column_name'] > value])  \n",
    "print(df.sort_values(by='column_name', ascending=False))  \n",
    "print(df.groupby('grouping_column').mean())  \n",
    "\n",
    "# data preprocessing\n",
    "df = df.dropna() \n",
    "df = df.drop_duplicates() \n",
    "merged_df = pd.merge(df1, df2, on='key_column')  \n",
    "df.fillna(value, inplace=True)  \n",
    "df['new_column'] = df['column_one'] * df['column_two']  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5d1c6b",
   "metadata": {},
   "source": [
    "b. Descriptive analysis and matrix calculation with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e42eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Descriptive Analysis\n",
    "print(\"Descriptive Analysis:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Matrix Calculations\n",
    "X = df.drop('price', axis=1).values  # feature matrix\n",
    "y = df['price'].values  # target vector\n",
    "\n",
    "# Covariance and Correlation matrices\n",
    "print(\"\\nCovariance Matrix:\")\n",
    "print(np.cov(X.T))\n",
    "\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(np.corrcoef(X.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deb21e5",
   "metadata": {},
   "source": [
    "c. Data Visualization with Matplotlib & Seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7437408",
   "metadata": {},
   "source": [
    "Then we will be exploring two important Python libraries Matplotlib and Seaborn, that are considerably used for data visualization.\n",
    "Matplotlib - It is one of the most popular data visualization libraries in Python. It's a low- position library with a Matlab like interface which offers lots of freedom at the cost of having to write further law. It’s an excellent choice for creating simple plots,multi-plots, and handling different kinds of data visualizations. \n",
    "\n",
    "Seaborn - It provides a high- position interface to Matplotlib. It uses smaller syntax and has stunning dereliction themes and a rich collection of visualizations including complex types like time series, violin plots, and common plots. Seaborn works well with Pandas DataFrames, making it easier to parse your data and produce beautiful and instructional statistical models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a4b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pairplot for visualizing relationships between features\n",
    "sns.pairplot(df)\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Distribution plot\n",
    "sns.distplot(df['price'], bins=30)\n",
    "plt.title('Distribution of Prices')\n",
    "plt.show()\n",
    "\n",
    "# Box plot(Categorial Columns)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(x='location', y='price', data=df)\n",
    "plt.title('Box Plot of Prices for each Location')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(x='area', y='price', data=df)\n",
    "plt.title('Scatter Plot of Area vs Price')\n",
    "plt.show()\n",
    "\n",
    "# these are few visualization steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbdf627",
   "metadata": {},
   "source": [
    "d. Feature Engineering with Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083dcb0c",
   "metadata": {},
   "source": [
    "Feature engineering is the process of transforming raw data into features that better represent the problem to the models, enhancing their performance. Scikit-learn, a Python library for machine learning, offers several tools for feature engineering:\n",
    "\n",
    "Preprocessing: Standardizes or normalizes features.\n",
    "\n",
    "Feature Extraction: Extracts features from text and images.\n",
    "\n",
    "Feature Selection: Selects the most informative features.\n",
    "\n",
    "Dimensionality Reduction: Reduces the number of variables to consider.\n",
    "\n",
    "In this section, we will use Scikit-learn’s functionalities to prepare our data for machine-learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c11e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Scaling using Standardize features by removing the mean and scaling to unit variance\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Feature Extraction using Principal Component Analysis (PCA) to reduce the dimensionality of the data\n",
    "pca = PCA(n_components=2)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94d8ded",
   "metadata": {},
   "source": [
    "e. Regression algorithms model training with Scikit-learn and Pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742f33bb",
   "metadata": {},
   "source": [
    "Training Regression Models with Scikit-learn and Pickle\n",
    "\n",
    "Regression algorithms predict continuous outcomes. With Scikit-learn, a Python library, we can train these models efficiently. Post-training, we can save the models using Pickle, a Python module for object serialization, enabling their reuse across platforms and sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8392e702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n",
    "\n",
    "# Model Training\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c7c057",
   "metadata": {},
   "source": [
    "f. Evaluation of machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8daba2",
   "metadata": {},
   "source": [
    "Evaluating a machine learning model is as crucial as training it. Model evaluation involves assessing the performance of the model in predicting the outcomes of unseen data. It helps us understand the robustness of the model, its generalization ability, and how well it has captured the underlying patterns in the data. Common evaluation metrics include accuracy, precision, recall, F1-score for classification tasks, and mean absolute error, mean squared error, and R-squared for regression tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d4d673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
